{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May  3 06:10:37 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H200                    On  |   00000000:1A:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             79W /  700W |       1MiB / 143771MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gathering storage and job accounting information for user: wchia7\n",
      "\n",
      "  ** Please note that the information and display format of this tool **\n",
      "  ** is subject to change and should *not* be used for scripting.    **\n",
      "\n",
      "\n",
      "\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\n",
      "                                          Welcome to the ICE Cluster!                                           \n",
      "================================================================================================================\u001b[m\n",
      "\u001b[94m * Your Name (as PACE knows it)          :   \u001b[mWen Han Chia                  \n",
      "\u001b[94m * UserID                                :   \u001b[m3370265                       \n",
      "\u001b[94m * Username                              :   \u001b[mwchia7                        \n",
      "\u001b[94m * Your Email (for PACE contact)         :   \u001b[m                              \n",
      "\n",
      "\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\u001b[91m=\n",
      "                                                  ICE Storage                                                   \n",
      "================================================================================================================\u001b[m\n",
      "Filesystem                                             Usage (GB)    Limit\n",
      "Home:/home/hice1/wchia7                                      23.3     30.0  77.8%   \n",
      "Scratch:/storage/ice1/6/5/wchia7                            176.6    300.0  58.9%   \n"
     ]
    }
   ],
   "source": [
    "!pace-quota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install YOLO package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /home/hice1/wchia7/.local/lib/python3.10/site-packages (8.3.80)\n",
      "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from ultralytics) (3.10.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from ultralytics) (11.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from ultralytics) (1.15.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from ultralytics) (2.6.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from ultralytics) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: filelock in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hice1/wchia7/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /storage/ice1/6/5/wchia7/conda_venvs/lizard_class/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO \n",
    "import os\n",
    "from IPython.display import display, Image \n",
    "from IPython import display \n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.3.80\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "print(ultralytics.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model's Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object Detection Model Inference\n",
    "\n",
    "- Will select object detection model of choice\n",
    "- Run inference on test dataset\n",
    "- Output will be saved to /inference/run/detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_789474_jpg.rf.45bd340a8e44e2c19834b5b80961e580.jpg: 640x640 1 lizard, 2.7ms\n",
      "Speed: 10.4ms preprocess, 2.7ms inference, 75.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_791411_jpg.rf.c695dbf6b3be317b0871b9cdf41c9e75.jpg: 640x640 1 lizard, 2.7ms\n",
      "Speed: 1.2ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_7995064_jpg.rf.dd34031b259231154d644d4a8cd6a5e2.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.7ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_7998833_jpg.rf.26700d552f7c0703f9335d3136b967e5.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8004416_jpg.rf.451418543bc911ab8ff41de884acf42c.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8015088_jpg.rf.6bec66e5707733cfafcac5eca7799f18.jpg: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8021470_jpg.rf.ee6f8167a22ff46b3fe28092bf8375ad.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.7ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8021471_jpg.rf.718ad1dbd7b49e35066e2e6597fa0818.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.7ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8021472_jpg.rf.5a4203b0a0a438141b951a61acf75820.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8021474_jpg.rf.01161d764d636dcd32e192a7620b025f.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8021487_jpg.rf.1d417153fadc87d08621dad327d3dcfc.jpg: 640x640 2 lizards, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8021489_jpg.rf.787a0eba1a53f6933f002aa4076b39e6.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.7ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8021490_jpg.rf.c623b0d4878f9fd8ad6813c12df642b1.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8021504_jpg.rf.e47bd5e1d38f1aa1159da6f89cb8b7fb.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8021506_jpg.rf.11853f8d200cfeaf778d9bbc26064872.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8021508_jpg.rf.a4677aadca835e4cb88ba8f751bc5727.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8040024_jpg.rf.539931a196e691774998cd5fd2af164d.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8052607_jpg.rf.e18477e724daa39872775c6b0344dc64.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8090096_jpg.rf.1f2336902f5354bd1490590d87a728a5.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8090097_jpg.rf.b00ca86d462b69ec84ba602873f475f4.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8094601_jpg.rf.add46d6f847e5802a8974bb221fd9771.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8102044_jpg.rf.68c52487526166cb4f061f9030f3777b.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8104385_jpg.rf.33d2efd72036fca5023e679a88edbf2b.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8109665_jpg.rf.cc63dab54f86b99035b84f1d7655a72d.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8154256_jpg.rf.fea3e3ed939bdf4824d9143bd128d305.jpg: 640x640 1 lizard, 2.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8177148_jpg.rf.df2b52b821277f7ebf0e37e5049953e8.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8196983_jpg.rf.c814ac660aa3ffd3acdf8b6bbc8ed4e8.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8202830_jpg.rf.07d30557b2a188eb1099de35f9e0718e.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8205568_jpg.rf.62601229ac8da1e50671edddc5149912.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8240279_jpg.rf.a875bfcf9cb227c023121bf9f64a2a31.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8244333_jpg.rf.21a70486f4bec2c273cfa1d12f61b5f1.jpg: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8255769_jpg.rf.30a7f14e0c8b50418eeb4c93f512540e.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8256607_jpg.rf.ff167969d46f9bc36c408274e2485f8b.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8283720_jpg.rf.45fbd9b8fac5e8a89afa7445a8fb397b.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8341197_jpg.rf.60acffd161ead5d1fe7d2e70d0ba788d.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8342454_jpg.rf.95ba4c6c27052eff1f2461dede1dfa05.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8351529_jpg.rf.2645f7b6dc7e6d42e90f0dad7a2bab49.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8354267_jpg.rf.69ba58c5360d684998fe33a70a815d9d.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8373840_jpg.rf.38ea4beeca70c2af57d31b0cf5d6a924.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8401152_jpg.rf.f2d8acb7ff32fe08e9c328c9ec9feb66.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8401165_jpg.rf.1a4662fc340d1931595f67b0cffd682a.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8431356_jpg.rf.62387ba1fd4f02b3194765aa747ccaa0.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8441726_jpg.rf.2a383110f2f565c7c56ecf9edeed1c7d.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8474505_jpg.rf.067a87583d0695456ca1e57ca84f2bfe.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8486752_jpg.rf.e5830e12aa051f01802714428a23b7f0.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8509631_jpg.rf.fd27e9755f872828b62acca36512da1a.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8515228_jpg.rf.817496c0a748903aea52f50a6b6b7195.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8519260_jpg.rf.454c1c950aa62ee75bb9ec63b74916d2.jpg: 640x640 3 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8534645_jpg.rf.97179578506868b6d7b51cf784776793.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8536736_jpg.rf.1e1bd17f90bbad9c70d55683234e3bc2.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8537431_jpg.rf.a7f1e402fb5896403232de943a7fabc3.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8539160_jpg.rf.db5c2f738c981831cd36ca55f1574334.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8539303_jpg.rf.e238a8ed07fef59f52fddb1d96fb177a.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8556649_jpg.rf.52450dda8a8be1651e54b68c993fe73e.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_861638_jpg.rf.ef491ea779d6011f4d29ab91fe1e03ce.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8620968_jpg.rf.26b8e3513c40681c765649d5d9681fa0.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8651482_jpg.rf.1cdf3cd66af2a524e3ad89df8406dfaf.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8651485_jpg.rf.06f25d92cb6d93a6d83e82a762a1e188.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8651501_jpg.rf.6f3daeca6ee766fff1371b9c3ca2ccfc.jpg: 640x640 (no detections), 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8656362_jpg.rf.c28c35345a46d3d810ba8af2ab95f999.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8674363_jpg.rf.ef75452d90b00b227cba5dc819be9e14.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8683061_jpg.rf.eedca017e2bb736257205a604fa8dba1.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8697648_jpg.rf.6e8b3615a418087ad5e3060ed577cfc5.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8710880_jpg.rf.9ec7d6f1be132565dfc88e7d72aa46c0.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8712644_jpg.rf.33efc7c09d8d1b7d3c39ec2d746495be.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8712646_jpg.rf.ea5f91b1b63d2cc62a383c5d3a7d6d2d.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8722799_jpg.rf.2fbc740f1059c21e1d1e8712687d27d6.jpg: 640x640 2 lizards, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8727766_jpg.rf.1659388339fb6eca9eb0499b202b20cc.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8734017_jpg.rf.5fbb0cf08c53d1570d4efa92774d4e56.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8744452_jpg.rf.80a7631e49028e58a53eac098dc867ca.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.7ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8753687_jpg.rf.091b112ccb2125aa4107069c12d97223.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8771003_jpg.rf.00cb911756d057f1c66cf84e360a0c1e.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8781562_jpg.rf.7603d0582d30ff1a931ff0032400fb10.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8792644_jpg.rf.ab1e0de0408319184f73d6cd192f9981.jpg: 640x640 1 lizard, 2.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8796175_jpg.rf.f3ebb6a1a8df39394a60ff35ff050d36.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8803647_jpg.rf.a1bca10fa12eb318a9742ab6c1eb54a9.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8829018_jpg.rf.3dc294d238f8b8871a9258683e67bf6a.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_886163_jpg.rf.a9d38cb8d58b597a856c6b3f0d2f0790.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8867086_jpg.rf.3e7fb2d0d2d0aff573de9b552936107e.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8871956_jpg.rf.4d63aee7df98a4a40ca8088b10feb532.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8881140_jpg.rf.016b8a9824002191a8df36a1759e9e4c.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8892807_jpg.rf.82635f476cc23779bcf5fdfa9834e683.jpg: 640x640 2 lizards, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8896385_jpg.rf.fab2c2b2fce757f473558c2b18b26242.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8934928_jpg.rf.a508f1b6c4bbaf08f1e30809c7b0ada3.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8935857_jpg.rf.20073067e373ea760b27d660a3c8b137.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_8990204_jpg.rf.4c4f510f7673b6664517c0eb4979a534.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9010066_jpg.rf.740fd6914fef149af638c6c0dc1ec612.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9011509_jpg.rf.665cd48728ae75519bac97729af5bde7.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9030857_jpg.rf.702ac0c561a41b52e18e5dfc23fcb745.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9032576_jpg.rf.1c6fb09e582b98ed9cf89e3aa3e3b039.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9049155_jpg.rf.d7e440b3fbcab9204c58c0a191efd537.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_905499_jpg.rf.9ae92b0ce16c6d1504d555af164e4aa3.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_905536_jpg.rf.96a4643ca0a8ce9edac553bb23a0ef0f.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_905559_jpg.rf.e0764817019290f75f32465dd991c679.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9064930_jpg.rf.ba94b3a3115f98a211059cc3eec5f1ee.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_906838_jpg.rf.68097837c244e2b9ff9c53ec6a7b6327.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9074806_jpg.rf.6cb90191da934ed2ac8d877deb8e4773.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9087154_jpg.rf.edfb4122770ee68bf13974f2508f4bb1.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9087555_jpg.rf.216a9b37686ea914fc65c6395721582b.jpg: 640x640 1 lizard, 2.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9141327_jpg.rf.1030dc48b4ed45f292f656e6ab3d2809.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9141338_jpg.rf.5835d701c296edec50475b5f45cfd6af.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9142112_jpg.rf.8833e1cf832e264c140ebca1b1457303.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9145435_jpg.rf.cf49345ebc732f66207a70468a48479d.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9163242_jpg.rf.45cbf5bd3fa6e4bac064752dc5a54efe.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9172305_jpg.rf.cccbad4d94d18ff050252f783c6f57db.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9181731_jpg.rf.78ec1dbc3757529d3d257883225805bc.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9214333_jpg.rf.d02710885de5dfd4130c93a1decc8542.jpg: 640x640 3 lizards, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9218211_jpg.rf.2c3f47edde16f41be0845f74213d3146.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9218671_jpg.rf.9a6a95148c13973427fd6876bb0e2ae4.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9242955_jpg.rf.b47e22ea9180cc8d58088774d1aa1cca.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9251242_jpg.rf.acfbd3070b5b744713fcb011166eb6f7.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9252760_jpg.rf.aba6e52edaa3236a0851be4c6d6a9579.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9253104_jpg.rf.18b2d7e7dc148b2fa0f6e455630ab05c.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9261937_jpg.rf.7eac09bceb7b65e23c217a4f3e3f0b46.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9261989_jpg.rf.356dcb4c526217818efd42fa4a10c319.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9263060_jpg.rf.3da5740cf4370251432563d1b8507f7c.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9273447_jpg.rf.f06ded89bcfe76b53f9be25c61c477b1.jpg: 640x640 (no detections), 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9275232_jpg.rf.f320f5413b4c5d8213859da09debdcd6.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9275419_jpg.rf.4109dfd4c7a1ea0645fafb88fa81c147.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9275423_jpg.rf.99788cefeaf210c5212910a7e2b45306.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9275464_jpg.rf.7ad2edd16b33ee0a5d3be18887c45488.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9282151_jpg.rf.242673ac2c0ccc5f6accb89ced4f24e3.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9287485_jpg.rf.720cd14a1f8b2425b2e6a7cac8cd5352.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9298632_jpg.rf.5673a3fe0796b6e29aba713118549c9d.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9301325_jpg.rf.b47fef19336ad246f32b5d1e8940991d.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9323175_jpg.rf.a5b7db3fb5a2ccb670d2035b32d96fc6.jpg: 640x640 2 lizards, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9326073_jpg.rf.15c7873024137861b4953ae562344b9d.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9355175_jpg.rf.8bf1d291b97ed471adcdf9d30f0d0757.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9355178_jpg.rf.45a2046c7c4d5d553dcc9f02708df444.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9360917_jpg.rf.c91d341e56970a734bed42d191dbe39e.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9378220_jpg.rf.1bbc07be4f79087f3ef7c8c3fd72e84c.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9379665_jpg.rf.efd9302d49d6f221419333da30ae7fb9.jpg: 640x640 3 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9380051_jpg.rf.37ce3d8f32157e7a9921ff5ba9ffeded.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9387825_jpg.rf.61d9161e05a90731ec7208cb7ca2ae4d.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9394090_jpg.rf.18aafff71365b5af079f7229fde8ebd9.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9449061_jpg.rf.6e4e9fbdce67558b7546bad95c428292.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9449523_jpg.rf.6e7fb3ebb9db0bb2d225f63313365da8.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9449527_jpg.rf.59b65a69495a1c25fcbcce576eb332cc.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9477108_jpg.rf.f3b0b0c79a8490e6bae643f76b62e06c.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9477854_jpg.rf.9f6e71bd36c759f3ea740da385e4c164.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9478068_jpg.rf.454c60049c9ce0bafa7fd09bc656f610.jpg: 640x640 2 lizards, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9510976_jpg.rf.18adebcb8af45d12fc0f018b2a935441.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9534796_jpg.rf.0c5913e7a6ff33f48ff8d1c6595e0d68.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9535656_jpg.rf.904d79f29a9df45f2b40acd451360458.jpg: 640x640 3 lizards, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9542318_jpg.rf.6bd1d3ca6eba3d43061a293d9f0d9de6.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9545854_jpg.rf.a0720388dabeebdc61a16ff38bf8f326.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9556851_jpg.rf.d30bc99451063758c32fa9d20d07b37f.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9564373_jpg.rf.c3505671ddcf00bf0f43fcdcb7b07a67.jpg: 640x640 1 lizard, 2.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9564433_jpg.rf.7b7ff7728512291384f5903e9303becf.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9565133_jpg.rf.628f6826230685583adf0f139e97baf8.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9589885_jpg.rf.0a5ab466b0f790b7d04aeb4c98a7d4d6.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9602678_jpg.rf.4e5001cc2a0f247ebe7ce8d14d841c42.jpg: 640x640 (no detections), 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9603003_jpg.rf.42ff55225e22dcd666948f865333c89c.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9605729_jpg.rf.a9ecf4e709f434e0706a7a9a1dcacf75.jpg: 640x640 (no detections), 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9639176_jpg.rf.53b8f3240903b8dd50118492853944c7.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9650944_jpg.rf.b1db315101cdddba23b4f11373b6696e.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9651545_jpg.rf.e0a797a19dfb83e41a825060b01666c1.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9682257_jpg.rf.f7b7eeef591b6b6ee71e03a7e9230c5f.jpg: 640x640 2 lizards, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9683370_jpg.rf.03e47fba64328562c90f8a826f9213a4.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9694781_jpg.rf.02fbf69f2da2e1dea26437506280c6cc.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9698482_jpg.rf.3df262dcd65bdbe1f9e469de1bddf078.jpg: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9704404_jpg.rf.4bd26bdf338f7bfefabdf869c56fa772.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9730109_jpg.rf.bdf3fa3a068759bf33756e928713bdb9.jpg: 640x640 2 lizards, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9740865_jpg.rf.402e77065a60bf29adfa6a4c10e1416f.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9754301_jpg.rf.1999c7a3c5e4d17125f6e2154d7ddd90.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9754505_jpg.rf.89c2d928de5f67da606f138b12403dde.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9757088_jpg.rf.20b2a5fa1193eb3ae13a0d4cd7d06555.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9781731_jpg.rf.d39fa86bb0d3036a53536331e24a98ea.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9782182_jpg.rf.9d01c69a2e5f22668d4c9348e634054d.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9782396_jpg.rf.777db99ef2d4c98a4e8fae699de5c94d.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9782476_jpg.rf.c662d222881da7adb7bc85402b578a46.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9783553_jpg.rf.98a4f1258271986bd1504d796b2099f4.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9796520_jpg.rf.d04ed1d0b07e0bdda9835262f2446af9.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9796816_jpg.rf.35d84d7be28b27044e39423c05bf974c.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9809746_jpg.rf.add5e5a46fd48ab431b408afe4a01fd5.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9809992_jpg.rf.59774ad8976b6a234764ff2b3e11f317.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9814499_jpg.rf.628d9d8768ec7aca5d16476fee30c36e.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9816188_jpg.rf.0d17fdb546af8bcaa16392652f6cecf0.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9823772_jpg.rf.d736264b0f62aafd7968a998398727ae.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9824042_jpg.rf.985264743ce266f74939152d7eb523e2.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9824952_jpg.rf.2d57b65fea2dfd21841d692b597f9b3b.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9824978_jpg.rf.d493637e3c583aa413a4578cdb58d5ae.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9826357_jpg.rf.01c44a1c02d2ded065e3073878aae350.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9826599_jpg.rf.f1265acc91797e0a201d25d0488427e9.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9827303_jpg.rf.46d7b63ccebb9b89074d134e04a017fd.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9847105_jpg.rf.3e808087bdee1ccba1678c41b94dd9d8.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9849877_jpg.rf.a2923ec4c8063097ef906b73ed2b4d7e.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9872960_jpg.rf.f65bcd90a511eb84424971c846668705.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9875775_jpg.rf.d8c0f21d1c3480be3037b304adfddc1f.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9876067_jpg.rf.36854ae9dada364b65caf60667baf979.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9878295_jpg.rf.36fd37d2ea370683de495b99573f5b1d.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9878303_jpg.rf.3daa55174c6f10de626868f0bd5e6a35.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9878970_jpg.rf.6ee01d4759cbfea4900f95f6fa48295b.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9879625_jpg.rf.4a8a8d4b790c412a767527e2c96b76b7.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9879678_jpg.rf.f1cfda6e54af66a66087ea8c2f16684e.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9879680_jpg.rf.36bc9da327ca11585484b9f7df8f676c.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: brown_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/116461_9879693_jpg.rf.2a38bcc643772e07eda0033514283ab1.jpg: 640x640 1 lizard, 2.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_75619111_jpg.rf.b22be798b0f97841d57ccb822d26f2ce.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_75652997_jpg.rf.4d9b916ae4bcfde5353a849253ae8fc2.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_75708778_jpg.rf.5909eb04b2dca3bc06448866d1b48fd7.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_7586141_jpg.rf.e2455b787bd91590d15645f0fc79ad4d.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_7586214_jpg.rf.5799734e988d19becc423d4e350f27c3.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_76008700_jpg.rf.1c6ffe3c75c6da723296b77e4b3342ab.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_76234103_jpg.rf.b77aff900ae7b7cb7ed65de7d023a6b4.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_76595056_jpg.rf.555534ad4df82fe303f76dc0cf30af74.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_76742366_jpg.rf.72484d8294104603fde6921124a0f403.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_76820471_jpg.rf.1fa4b39adbecb56625911c4b1967090a.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_76936880_jpg.rf.1bef4e4fb833e52f2eab33558381329a.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_77005589_jpg.rf.d56d6b1101e41f89f1e1d846e3e8d658.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_77082780_jpg.rf.2a42036aa9f0e568fc0e3a6f8e373c13.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_77184263_jpg.rf.e41dc38e412bf87481a834d4f1f191f9.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_77636805_jpg.rf.2fb106199df58de5873ded4b4ffc61be.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_77649691_jpg.rf.faef2fd86236d15ddf96a00bb193eb35.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_77652627_jpg.rf.b2ca3e05e28b9855564049ea0f63097a.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_77652910_jpg.rf.f55efb88e27fd7e0428c600e60a5d745.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_77776146_jpg.rf.00b70501ca3f52fab5c46bb0f77f35d5.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_77786137_jpg.rf.05f072fc7bfba6867189c0d6a886260d.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_77913282_jpg.rf.625d886df374474a3446506a57062d81.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_77977219_jpg.rf.e00be4c60dc28ae19785f864ab0a7194.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_78347858_jpg.rf.e1bab3ef468926a7a75c9b6484acd589.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_78376457_jpg.rf.b9b4c099e8cc9ed112b1313fd76cec44.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: knight_anole\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_78481810_jpg.rf.5dd54d12194ff236fba537a5493b43bf.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_78498048_jpg.rf.4f60dc4523dde2ce80569be072669888.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_7896282_jpg.rf.8cc213bcb9a9609de7718755d48f1b85.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_79294660_jpg.rf.5f92a2deba729b5a06af329ae52fe29f.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_79426531_jpg.rf.7b7c3410f9a5cf1701ca5de949787dbe.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_79714320_jpg.rf.3419ad59d586bf9af32ee91415cadab9.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_79725432_jpg.rf.bd8adf45bf1b28654c782112e5d487b6.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_79980783_jpg.rf.643c30bc4804d477e208946ea1fb2002.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_80067601_jpg.rf.1f327c776c15f50b4f4e006475285b99.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_80090480_jpg.rf.df71872c528c0144c22b504d0a397899.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_80125462_jpg.rf.72f7fe40b78c448e7858d0a2ad8f8bf2.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_80232568_jpg.rf.b9d6508d220b14daac376b6a1675a253.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_80349217_jpg.rf.e1518f95b4b536251607f9e2059e4911.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_80362955_jpg.rf.dbabbcbf857c2809bf24c6b676420dcb.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_80741615_jpg.rf.9052e2cbbbfe0155f9b38e8dcb46dbdf.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_80879714_jpg.rf.1078d222d2948aa008b2da1b044a5484.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_80917467_jpg.rf.69fedb998db5be3a55b702ba6d3e43e7.jpg: 640x640 3 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_81138504_jpg.rf.ead8dfa249b2f510b2b1fd9727390d73.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_81208165_jpg.rf.f7fc1b67db200f9fe62bd5b3ecf2a5be.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_8122599_jpg.rf.de240951feb77eeb94523936be05d98c.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_81309182_jpg.rf.cb8b84dffcff345c45c68e85cd8e6fc0.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_81351520_jpg.rf.9da061ac823343e803e475060f1487bf.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_81390602_jpg.rf.da6519376d76bb760c358cf376b54df8.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_81505029_jpg.rf.3ac51e594f145c89739557079add1892.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_81774038_jpg.rf.889b6a6065d6cb7ed13d6a6a01f5a16a.jpg: 640x640 1 lizard, 2.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_81948771_jpg.rf.d5c22ac95407f28a64a717468b97da10.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_82053467_jpg.rf.39661d07256f3997093480a492ceb33b.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_82450427_jpg.rf.1a93d64c477e3400f623a20767818c0e.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_8249871_jpg.rf.99bbf87cfa88e67cdc991dfe26572581.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_82545095_jpg.rf.51734047696b89bb784d50abeff39e85.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_82735486_jpg.rf.62f8896bd710a2ddca31009365fb99f4.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_82799705_jpg.rf.70f1d09e019a39c5b025a13b715f25b6.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_82950001_jpg.rf.aaa9d3d707ce1f77a9ced4490e6a872e.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_8295351_jpg.rf.4775db92ddc6bbe4aed0019b0168f178.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_82955589_jpg.rf.af0d28c7554d32b8925fe58b0bee4e8d.jpg: 640x640 (no detections), 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_83617274_jpg.rf.715ea8493a5acc285cfbafa2b9c1bfb8.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_83669178_jpg.rf.180ca6e5021be757737ba1ca235f450e.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_83669531_jpg.rf.7c0ffd185f32e58a115a463c272fd2ee.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_84112331_jpg.rf.2987312c88ba035e323f914d014f637c.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_8417286_jpg.rf.569c50a2af5988f9706d5c261d9f3b97.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_8417379_jpg.rf.21ad88974d68c3c5f0ca218f3bb9077f.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_84331639_jpg.rf.239c55ad68a96a08fb9c8e06777e0cf1.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_84331854_jpg.rf.5260b7e18c94d9c7857cc868bdaf79c1.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_8456214_jpg.rf.3bc1a1997d0828d61f4354c4190e34b4.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_84659232_jpg.rf.58816f89d235eb9f12bd84ecb6909bf4.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_84714837_jpg.rf.ec65e39baad9eb887f8243d0c7783288.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_84742854_jpg.rf.f503bfcbe6b48432caa6c9874775c360.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_84967884_jpg.rf.525f116d5e1ec368153f8ec555d9a133.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_8519675_jpg.rf.fa84d5e3ef2d4d3d9c6b0562f0b1ede1.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_85233342_jpg.rf.e2fd9c9ffb9e09d3b31036ab92d6cc9e.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_85341938_jpg.rf.4c09443085f3053df4d9d5d9d29654b1.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_85697242_jpg.rf.648a3c19dc9e2bf58d3de8185277eaa0.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_85779050_jpg.rf.152de175c4787247ad4a4d4eb9217e21.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_85929116_jpg.rf.b123b44fb3e84f6df71e27a5a9ce3fdc.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_86118341_jpg.rf.b5888c87ff6603a59eb7bd729746d9ae.jpg: 640x640 2 lizards, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_86207200_jpg.rf.7da948654b32ac63cf9fc2ac03f567b3.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_86415651_jpg.rf.b8f75d7f5f228fd8e737fcd230b0d517.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_8644449_jpg.rf.4c2722d4f2151338da86f80d14ee7c9d.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_86565566_jpg.rf.87086efef60895916729f627b6615bd5.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_86651551_jpg.rf.0fcf7d4e767da8370b3e32faf57d0140.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_86704357_jpg.rf.62cb2f0abad23e1bb7d1c32f23c2ca2d.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_86910665_jpg.rf.24018a4a45fbc80f7e2af6f798c5b8ae.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_86947063_jpg.rf.1ee18732813d475b521acfa4e8788b78.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_87295947_jpg.rf.f4a4514840b087c7e9fb72536fc13e95.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_8760300_jpg.rf.7b064a0fce17f6372a2772da483eb363.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_87609698_jpg.rf.ff5cc949c159b485c49c89fa264b8bd3.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_87879250_jpg.rf.d8d77753c5fcc0eb086800e6fb473016.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_88021870_jpg.rf.fe3d91b1378c9ad5ac41c475ede19ce3.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_8803704_jpg.rf.0531d3c8b8d3423a2c5d458431c0a953.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_88125698_jpg.rf.ef881b1ca15ecc4dde7ec2b4b0cf259b.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_88128993_jpg.rf.fba233ab8389dba279b143241a33f349.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_88174141_jpg.rf.59ff4143fb7fa04f01d859ce9ba05a88.jpg: 640x640 3 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_88254850_jpg.rf.51e8f0516938011fbfa187317a9c43c6.jpg: 640x640 1 lizard, 2.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_88264657_jpg.rf.e83672b33b283a878a33b3d7a25f4760.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_88317341_jpg.rf.b00b9342a3e5ab33627d5df7b928b335.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_88578072_jpg.rf.443bbe95c31003cac8d03f28386d9e18.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_88639193_jpg.rf.3999b62845ab0ccc86fc6f211595441c.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_88706541_jpg.rf.d06454307f1e4f33426771cf60a98a58.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_89333279_jpg.rf.d38dbc6bb36ddc0399af1874ee163ce8.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_89424944_jpg.rf.4d260abc1a883fa029811b1660918adc.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_89799786_jpg.rf.52c04c30e2685c48c36dddafc37e1d8e.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_90195159_jpg.rf.7965eda3231ca5b5736853d2e858941d.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_90260455_jpg.rf.c7954661925983cc9c5373b92fa3177f.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_90281558_jpg.rf.3288170a1d6c83a3e4fd468671f9be66.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_90288011_jpg.rf.8b078dd0545ecb7a6ad7c9ba5a1e0634.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_90344911_jpg.rf.4185fd61c6a8e0e62393cf18f0275343.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_90434468_jpg.rf.81fe1919ccf04589664ce5ef5dc0074e.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_90448964_jpg.rf.42d13c8e5aa9707dd68f54ec077a21d1.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_90528264_jpg.rf.abdd16b071128018e100ceb183acf262.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_90586845_jpg.rf.9e88986b0c4e1534fb81e29857b6f3ac.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_90615218_jpg.rf.a043ba3740f9eb3abe68d9b94290d731.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_90939188_jpg.rf.683469118d5e8af481ce731a13ce2e42.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_91175789_jpg.rf.d3ce21d78d5b15a3631b3ae8077dd3e3.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_91198274_jpg.rf.7ee30959d3f3ffd0ae68014c0080eb10.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_91399809_jpg.rf.30d139956243aed396d14e6d6847cd72.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_9140461_jpg.rf.edaba46de93107923078fefe6bd13a34.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_91409938_jpg.rf.530c9cbaa0b7afcc6b631a9d8bf4c2b8.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_91435691_jpg.rf.bcb3a90ba09580ac6b4946470cc3206c.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_91535555_jpg.rf.9c365ab68fa85b29b0bc2aa1e29038ac.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_91547322_jpg.rf.68663d3172cda47d97141429a0fae2fb.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_91740056_jpg.rf.4285c15d0aa3dc8914fdcf43c1917fe3.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_91808600_jpg.rf.027fba6136ba82165d146e94a01c7978.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_92100970_jpg.rf.a5532b1617c836f6ce464a3e094adbaa.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_92388729_jpg.rf.967afdb8f43a87c0a8ca40251077017c.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_92468743_jpg.rf.7a4027961f68e9a6514ac087d4b55477.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_92495148_jpg.rf.48da1d7c640d5c3efd8fa8ab990f10f1.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_92560585_jpg.rf.385fa4750d819b743d268274588f3184.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_92637407_jpg.rf.78693d8aba9c893d7e7ccc0e9cf49816.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_9274850_jpg.rf.634d002bcbc582834824768533a19b9d.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_92824180_jpg.rf.570d7bc339ee0d8351c520c6908a31e5.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_92906293_jpg.rf.e55bec08cbe6863ebe79d319ffc85d0c.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_92971583_jpg.rf.fc65427c6ba5913831f8188abcc50ccd.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_93032064_jpg.rf.f10765fce817a0ca5f0ccae02ccc400e.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_93420825_jpg.rf.a6bf3a02062a8f3e52aece9b6ca8a4b9.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_93493930_jpg.rf.4a4fc9a4b6a6c3d91942f0ac990ce702.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_93773931_jpg.rf.5b26ed56bedc05b049a6961b2d282b9d.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_93777838_jpg.rf.b914108aeeed620d8745f5d5f092a088.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_93804485_jpg.rf.638136377287153e16e8fc4b9f0c9ae6.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_93869490_jpg.rf.fa1f656d47714971e1cbb477bbf36ae2.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_93886143_jpg.rf.aa281a6306b107894410896fbbd1f779.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_94101371_jpg.rf.4b02491dd99f4ce5811e8fed7c34faff.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_94391418_jpg.rf.52b95fc16d01cfc47e1f6faa6f616703.jpg: 640x640 2 lizards, 2.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_94470018_jpg.rf.56d9a676ce212de6f321ca9484d7af2b.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_94960399_jpg.rf.1962dc7e743da51c34b727ef90ee18a3.jpg: 640x640 2 lizards, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_9519594_jpg.rf.425345cb67d60c1dfa8d0310d91ef69e.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_95293926_jpg.rf.9ba531b504f9c336eb5d9cef73229ed9.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_95316317_jpg.rf.8ad30b69e751738579d5c4bc54780c09.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_9538375_jpg.rf.0070e2c7f3d065b4aa51418fef854dd5.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_95398091_jpg.rf.345f76cfe0183edbb302a1f213f7849a.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_9552565_jpg.rf.9111735408be5919429c7fa27a0e6cf4.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_95593411_jpg.rf.bb26209a3fb7b720423678e7844c6c81.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_95656610_jpg.rf.a442b030d9d1c00a911a5be50b6063ec.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_95797788_jpg.rf.f0157fbc135ce7c682bec0a4c93c4535.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_95889553_jpg.rf.8125525cdb5ab99d717f9f048d8e893e.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_96121637_jpg.rf.6194590afd66d82361e7250281da39e2.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_96262599_jpg.rf.c57ada2bb36a0bdec5a21b8b8df32b5b.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_96500283_jpg.rf.91f8762ffa29d3a097fbf0eba3469437.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_9663842_jpg.rf.ce41e8ac954aeb8a4d24cea3bc677e53.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_96732137_jpg.rf.a5d8e61bce368687b66d051858c1b821.jpg: 640x640 3 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_96986892_jpg.rf.903fe0b82e373eb5b64dbbbb4c8eebeb.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_97023199_jpg.rf.63c4f0ebbf009d4daf0e139f80502083.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_97032182_jpg.rf.7e82a59512d568070be1891958a84c95.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_97137690_jpg.rf.f6831f0435eb20cbcec7b75a6ea66ca4.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_97157609_jpg.rf.54a2448e0497265f576603bf76416d4b.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_97202051_jpg.rf.652dd3b2da39675dcb235e3cfb422128.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_97233253_jpg.rf.b79ffd752ed601d248781f04f83b303d.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_9731891_jpg.rf.9a5e3be2f726d032d3ac9c0a1b242902.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_97386129_jpg.rf.a383672a03e9921318a99f3e5eb39503.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_97562261_jpg.rf.c28924686bc34d2d152f8eddffd92237.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_97566123_jpg.rf.4debc32884e1c1aae89340dc8fe18b9d.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_97566311_jpg.rf.2eb25507e9b1848e44358a86c983b7c4.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_97575202_jpg.rf.7b17b068cdf3251def9f814ce9000f41.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.8ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_97648637_jpg.rf.15a39089d87f73567f4042699fe98b47.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_97896700_jpg.rf.c846cb98d7cb41cb748215f1603bcca4.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_98065621_jpg.rf.8ddb1daab6b2bad3a487728f9fd992ba.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_98122369_jpg.rf.4cf5674feb8949960c194855518830ba.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_98153489_jpg.rf.c21b22bb5eaa3e9a7a5835e4a0a09964.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_98157022_jpg.rf.d165e3fe3909e0f9dc442661da9a198e.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_98399706_jpg.rf.2378d83dee401c80980abb7bc42ce9c8.jpg: 640x640 1 lizard, 2.2ms\n",
      "Speed: 0.7ms preprocess, 2.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_98555728_jpg.rf.b122a5519b7b790f90e1cc07f85d9382.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_98563405_jpg.rf.bf2a05161ea10901d4bbf2d94a9dce69.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_98572641_jpg.rf.3447c4bf5f1f08a145e81f0614f36a4b.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_98653914_jpg.rf.30b927151d109a38832ad6d6b5e95cf2.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_98756219_jpg.rf.47b274e5652505b542fc793a33a2b5d6.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_9879782_jpg.rf.688523bdfe0f8dd43a7dabd14f75254b.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_9931138_jpg.rf.cc0f5985f7dadb33ae0a8f478f1ce351.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_9932020_jpg.rf.5f5ea1ef95f88f8eca6ed7872cb831f2.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_99389221_jpg.rf.04d3c141bfb7cf7a4212646f8afcb093.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_99444910_jpg.rf.39874e1bb27b788ab58fda4b89e3c0fe.jpg: 640x640 2 lizards, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_99453159_jpg.rf.7b5f56ee5a6325bfc8da051fe0de0303.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_99463529_jpg.rf.53504b2f321f12851084f1683d159921.jpg: 640x640 1 lizard, 2.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_99472126_jpg.rf.d02935f3644b08b7cc5b22a3fc8c2940.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_99557918_jpg.rf.cf6e3a6d76c1fb555f54ab15afedc947.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: knight_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36391_99853198_jpg.rf.a3639ee2d1c1237d970e0ca4d35d995e.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_224261493_jpg.rf.37e125a7b9d08ad7e749054ed9bd80ff.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_22529312_jpg.rf.1795f5999cae3e6f17ec57d4b027c575.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_22529467_jpg.rf.a19684b2535a850d012c949c0ecd6fe8.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_225821080_jpg.rf.dde26127f0a5ade6f8938273ba38a00a.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_226045741_jpg.rf.5275282fa5469aeb3abe806af4caea5a.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_226497687_jpg.rf.3605968704e60ff4b79ca59d14265eef.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_227104692_jpg.rf.22a86ddc3f243df99d6cae61ef511ad6.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_227104859_jpg.rf.b4003965b2aa1d8e3f42c586ffd21036.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_227173692_jpg.rf.c193495a616c2b6de06d92f1bed32f02.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_228822983_jpg.rf.5657e113ac8a9981cd2c3ac30b1feb44.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_228875523_jpg.rf.2e84e36a64e0939d6bcbcd61422db886.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_229108512_jpg.rf.f1dbd0dd5fd3b2f932642f344d51766e.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_229613189_jpg.rf.8cfcd89a327d59f8dca33cfd75f096c1.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_229628260_jpg.rf.6ece50a86ba147283e2ad40469c63876.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_229674838_jpg.rf.a452f3856eb3af580a846614154e321f.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_229738780_jpg.rf.b94da9560204e5c17cf982a45e4d5eae.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_229813993_jpg.rf.bc225631dfa80be3e38360cfe7a91f4f.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_230137383_jpg.rf.68ca53d3b6f38d22ce146c7e3f2282c0.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_230504988_jpg.rf.d239647ac8ec02dda0b9d88ddda1bc97.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_23076351_jpg.rf.55e7af197c81927136dd0845f77b99d1.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_23249450_jpg.rf.00a783460b3e77fede9703f9870a633b.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_233103687_jpg.rf.7e98b2d8772e47c8a18b046f9d2a0a5c.jpg: 640x640 1 lizard, 2.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_233339473_jpg.rf.90fd3746e43b46f883501f085d892e4e.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_233801097_jpg.rf.621910c13a1130d0e8636f075347e47c.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_233981046_jpg.rf.736d1c896d9db6e5028ad65154e8aef8.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_234158008_jpg.rf.ae66c4a7222f7231eb77bf2cb002e12c.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_234195226_jpg.rf.5fc94b8563ca5693c803688263c5a25a.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_234782210_jpg.rf.e5318e12b2f2e8bbd41d10135bd37d1a.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_234909385_jpg.rf.1ac597d8e33b3529f583d7bbb2897ec2.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_234938416_jpg.rf.1c7f37b8b289527dc010294044eb28e2.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_235237765_jpg.rf.4f0260da65e17a3556d52fc047d276ad.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_235585046_jpg.rf.9979b71bbb3a9cf54975dfd809bdbd71.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_235700958_jpg.rf.16d74ca8b25fa5d2201b71360248bbff.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_236328216_jpg.rf.75092188dc7f0b00f97acf6ecf68ff60.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_23637397_jpg.rf.123e571e6c09d24adba663ec11b1e240.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_236711235_jpg.rf.54a2984d4597b728738ac9c43ac84b9d.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_236774205_jpg.rf.fbb6215974a2ccb10d291a2a5260bf38.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_236958654_jpg.rf.8ad3407575fd25bb48befa07cf80f9e2.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_23750539_jpg.rf.20a64f20f68e9d755dc67d1d2934457c.jpg: 640x640 2 lizards, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_237520203_jpg.rf.1de477b1b440cbedae1221d243eb3fdb.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_238053730_jpg.rf.8bbdd38358a1739ba38d391971cab8bb.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_238136100_jpg.rf.bcaeb62a15e92e9283bc65ac06824737.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_238485387_jpg.rf.f55b82c458834b20216bfc334cd87e82.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_238590982_jpg.rf.4ffa720afd044525685ea05ff8b3b361.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_238801600_jpg.rf.df5676e57e63abe04fe65a2cde51ccb6.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_238871296_jpg.rf.92cb26ad0523a963576f6db74cfe69a0.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_24907484_jpg.rf.55ad3da922e1e13dc52d336e37a0805d.jpg: 640x640 1 lizard, 2.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_2560046_jpg.rf.64ef6a3e074b045ad48e4fded116bad7.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_26296527_jpg.rf.e5484b7d1cfb59db1d4ac26877c993bd.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_27057079_jpg.rf.c9a9e33d98c4838a352ce39527e9d819.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_28452937_jpg.rf.e1fd9e1a72a3fde4252d1457bc644394.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_28915634_jpg.rf.5625f0338292b4700d6987b0b43e3f44.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_2899169_jpg.rf.738439db1620e43cae38315b776bec6d.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_3007742_jpg.rf.eb25d34c72a0a132fb50bf1848456e4d.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_3058290_jpg.rf.128786b95cd0e91357d8c30a6ce142cc.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_308270_jpg.rf.c6e17763de574bf95e2c9a3355f0ad4b.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_3106737_jpg.rf.9030cbee251977d86bb52ee7d17c1375.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_328227_jpg.rf.6027eda77df0b24f39dbfbebc028b98c.jpg: 640x640 3 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_32833860_jpg.rf.1c4f8c768e9d3fbb9b22e24d835ed0b3.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_33238663_jpg.rf.cacb7185d48fa88c74f085dd0160b2f1.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_33533337_jpg.rf.4d973975b87c5f346a7b3660f4585f04.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_34508905_jpg.rf.9cf419602641ba5190346777b6ea16a0.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_34509377_jpg.rf.654b93eb795be141952530cfec5ff8d7.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_34510429_jpg.rf.c959d353a058e097f8dc8f82ca6a6cec.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_34510665_jpg.rf.a69d206419934bfcb8a5d563363f6135.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_34703952_jpg.rf.61d3ebe930b2f34716ea962ae19f2a05.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_34703977_jpg.rf.4039a2ab9e59be1eabe2a98781d8d55d.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_347121_jpg.rf.fd90b676a97f652d9f487c58ed683f4f.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_34809887_jpg.rf.de0708e3922176ac52b3a14562abb006.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_34810129_jpg.rf.c046315fa9822fcc6bc9b5c45412be18.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_34810206_jpg.rf.a44218195ac9581299bbd34966bc3174.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_34840425_jpg.rf.2fb23285a694bed4dd600f4c5149016e.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_34842937_jpg.rf.181428c8374c08781fb502c231595009.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_34909165_jpg.rf.5dce814e95ff1a52b6800a19ec63bb14.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_34909179_jpg.rf.39c20a0a4ed780f07afb075d9bbc6be2.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_34909283_jpg.rf.9456804b168a29c03a9f52deb31c777d.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_34909471_jpg.rf.e52ae3cc19ecc371fc4ef46a90536781.jpg: 640x640 (no detections), 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_34909605_jpg.rf.71c449acbc3d172eb6fa78af6257bc2d.jpg: 640x640 1 lizard, 2.2ms\n",
      "Speed: 0.7ms preprocess, 2.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_35240352_jpg.rf.d115dcf1d0dbba152608dfa8add08fb3.jpg: 640x640 (no detections), 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_35240391_jpg.rf.d3e3fdb9f8f10d0dc430ef5fdf32129e.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_35240572_jpg.rf.28434883195cac214263c3a892eeb86e.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_35391707_jpg.rf.8f085ae8989a0c45ebd35eaf8d2a7055.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_35733927_jpg.rf.65531785cfe5ba2993ca9c59c28940f5.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_36186050_jpg.rf.3b2b85b3e5fabbaf1d4f5773670f70c9.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_36353992_jpg.rf.93ba239108984a8f16fc524d88322e17.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_36354078_jpg.rf.e0197ee3b5203571f5091fdee58402c5.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_36512522_jpg.rf.4446beabf18112baec3a2ad9b50bdbe6.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_36683662_jpg.rf.3aaec979bf3b6ff717fa0cee56287191.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_36684297_jpg.rf.ba34d83f717f089a908a2159d73f2e9e.jpg: 640x640 (no detections), 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_36685681_jpg.rf.9a40b7aa9979059979ec4a2faf4981ce.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_36685876_jpg.rf.886b8aa640899c36319064e8ba3d692a.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_36922106_jpg.rf.c4532bfc5c065b2737d119d865cab363.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_37014103_jpg.rf.b8a2c3d77375277fafe198f9f053055d.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_37233439_jpg.rf.2b56138aa3986d50b652a4d783938f99.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_37310212_jpg.rf.00c67547a255b4776bfa0e363b1351e3.jpg: 640x640 1 lizard, 2.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_37338530_jpg.rf.728756215e65672c9db1bc4e6c7db038.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_37725648_jpg.rf.46b9ce856ec03b48ebc806dd76bbc218.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_3778474_jpg.rf.15acf7c5c121e2a7bca75c003ce5abb4.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38074885_jpg.rf.dd342722006da7378f44d7029159db1e.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38180729_jpg.rf.63e5b3f2fe00c5bc96f638b3db11a521.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38180804_jpg.rf.67d787a1a9d9b5d45992284d347942b5.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38181025_jpg.rf.a717a90156920c755ae3ec70bce2264e.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38181036_jpg.rf.e03b03d189c2a6f259d9b9959000fa7b.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38194095_jpg.rf.b2935d30520465ec1eff8c5443c7489b.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38223465_jpg.rf.55da96526823734345b1bfa4302950b2.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38231083_jpg.rf.10bf39fc62b7b9406c55ec0c2761eab4.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38268806_jpg.rf.3422dcb34b3f686959dc51585ed21b2e.jpg: 640x640 (no detections), 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38310203_jpg.rf.5a243dfc685cb27d37d774c13c8499af.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38327195_jpg.rf.c10d0875a8a3e7b185f1e141f4c532c9.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38335766_jpg.rf.62b0b43817a20b8b4676a1893f5d0800.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38336709_jpg.rf.e5ede4742920b74ea5695c4b8d4a75d1.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38435477_jpg.rf.d64bcf892dd262a1db8db87cecbe3f9d.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38466107_jpg.rf.eb47a02e6aeba66f77f802a5f4d29030.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38798946_jpg.rf.0b09856caa38e66414082bb11c5cab8d.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38798969_jpg.rf.fbac49dae1ace7ed3a03c7af65d427dd.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38804358_jpg.rf.145a20cbb5522c95abca3d4f29f9e8af.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38804753_jpg.rf.3daeaeec365885eb7415989399da5302.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38941128_jpg.rf.b3a00a9d0b2b6f7ca9411ca0c59836b9.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38941597_jpg.rf.ed6bd1c7cd264bea836d2f0983778a10.jpg: 640x640 4 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38941623_jpg.rf.c0aace0817815bb4f6535f913e65ef03.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38941787_jpg.rf.edd12cc4719a893f98b9e01ad5784e7e.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38942051_jpg.rf.6a44bd1c48cf7f1a41afb75721f4e81d.jpg: 640x640 2 lizards, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38942052_jpg.rf.8baa98632527c836442e606ded866a6c.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38942053_jpg.rf.d10d1639667763f76be994d9747aca39.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_38943200_jpg.rf.bfebcb50b7f82606abb5b477d381c570.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39152041_jpg.rf.bdf5f7b90fc73a5df4db37c12ff33693.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39153248_jpg.rf.43e801683524277c02109a262701836a.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39174531_jpg.rf.71d095e1d3da37546b3aac41ddf55803.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39178922_jpg.rf.f11f2cf46e0705802efc0c2973e95478.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39178936_jpg.rf.9f4593ae87f8cb17589769001c56046b.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39182400_jpg.rf.2ec04b2a0dc333d80c807c006b14f5ca.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39182601_jpg.rf.23fa30cc261a80c5e0bb6eb5306508ce.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39214413_jpg.rf.8ea92a772e376f70bb6c13cc255ecd2f.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39214433_jpg.rf.62528a2529c6753350958bafad729155.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39230087_jpg.rf.d924f81f1a9963c70da3bb011e4ada27.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39295597_jpg.rf.f942725292661327d2f74100847ad70e.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39296289_jpg.rf.c894e4246df249dda7643cbec47111d4.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39332386_jpg.rf.b8b46709ca3b0900d8c06b8546080a5a.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39407923_jpg.rf.d68833c8520c6a800667cd5fa28164e5.jpg: 640x640 (no detections), 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39411666_jpg.rf.c0edb47e124b33ddee4425c3fc270d50.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39647332_jpg.rf.c365d8a735f415c36b1f4c4eb1aa17cc.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39653159_jpg.rf.15d70afd88b6bd3b6fb7aef31c22f8f0.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39725629_jpg.rf.26e60e4536e1ff5d10ec75173d5a8890.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39728621_jpg.rf.e428a95491f54e380172b8cca5fee393.jpg: 640x640 1 lizard, 2.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39728655_jpg.rf.bf1a17e121ff2815ae239bd5b68abe79.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39728759_jpg.rf.1451c694f608e072631659324c3d7837.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39776680_jpg.rf.ffa41840021779e82831264baae0f66f.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39812509_jpg.rf.00e67b508d3d986b72312b84d93abccf.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39818381_jpg.rf.335c29daa66d19969988fce0440e8f62.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39818726_jpg.rf.592bb14404db3584a3225eecfd451379.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39836910_jpg.rf.3c71e05967df286f98edf72399e9a2ac.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39847635_jpg.rf.05b66d7793bd333f64758e310d620337.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39848596_jpg.rf.92089b5be399266a7e3aed264af25646.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39848694_jpg.rf.54cea907f9f9f950e504a475d9f25175.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39851820_jpg.rf.5d35e0b49cb55a89366eb8d623580ad0.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39854493_jpg.rf.9a12e18165eaff28019490123393c8b0.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39854872_jpg.rf.66ede704d1373c04848410e524188406.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39860568_jpg.rf.bfd5237bbba1fc932d3056b151d546d5.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39881829_jpg.rf.35540c6e04dd5be4ccf97480669b65c6.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39882093_jpg.rf.68e7061259aaf260dcfb7e36f117f31a.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_39977180_jpg.rf.32d6f4257337825cdef097d6bc7c1ddb.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_40046966_jpg.rf.1ba0e30a4c4086a0aacd5cf5c21ef9ac.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_40108398_jpg.rf.726576301af124c561a5e4c151c121f8.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_40406501_jpg.rf.c5b511166627af9250cbd57148ff8f5d.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_4064790_jpg.rf.6a04fb75f24edd974762cec370e4d1cd.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_40762015_jpg.rf.6ff11368dfb7f281b139d9449da3a618.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_40898398_jpg.rf.145f654039298141f1402034e39f4627.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_40910004_jpg.rf.22789c4442609bb03f2bda524db86814.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_40910009_jpg.rf.4f72a3837be6f4da72ce8d82eae87a22.jpg: 640x640 2 lizards, 2.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_40925350_jpg.rf.1cde1ce0a70de772c59c8a955fd476fb.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_41000203_jpg.rf.b50a1c1e806416f9bf13d7d5b8ebb0f1.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_41019896_jpg.rf.101dd49256aec3fae1bfee55aa1fe866.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_4108228_jpg.rf.dac6af43467b400ab53b7d5df959c623.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_41087963_jpg.rf.f148855ce7ad410dec9613396dea06f3.jpg: 640x640 2 lizards, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_41186042_jpg.rf.ea0651a3bcb547a611fd9c84b77b3a85.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_41186088_jpg.rf.2399cea6a63315d126f722783631bc5c.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_41312288_jpg.rf.34e4be79d1b5eb19a65d6cca86459309.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_41551295_jpg.rf.d84e66f70f2c1f6d6b8d38adaaa47cee.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_4173631_jpg.rf.82469a4dd9190d851fdd733fd3e60128.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_42195491_jpg.rf.03397f0a2a163454e2708d128d12441c.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_42195615_jpg.rf.db88d61e4e45ea4c7108c9d88ec377c1.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_42531363_jpg.rf.28b6476b8fc0d3f50118897b47e93503.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_42531370_jpg.rf.bf957b75e0dfab4bd93e896e3886c12c.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_42531371_jpg.rf.497f379f6c7041f3560b53b8c6efeb95.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_42531375_jpg.rf.9bb7cb629ddcd4cc5cbe2527b16c3c8d.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_42531379_jpg.rf.4f860f010d443fa50d12dca29be7dca5.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_42531386_jpg.rf.b1dd8efea672a95b9921ad131cd4ede3.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_42591031_jpg.rf.907abaa8d16ff86b2e74e67b8263c2a6.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_43156644_jpg.rf.fa90f1c1ea4290c5cbf3544db1b6cce0.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_43504729_jpg.rf.009eb3ea155526ac7f9669a71fbc95d1.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_43506578_jpg.rf.5cc6b0189ef781099784ceac1e0a4137.jpg: 640x640 3 lizards, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_43656607_jpg.rf.88804a5e407c5e1fdff3ba7955180d4b.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_43934817_jpg.rf.46a617c7e40004a959889dccc3dce131.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_43949097_jpg.rf.6a411844cd12066662cbd2c7ce7aa3df.jpg: 640x640 1 lizard, 2.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_4396728_jpg.rf.5225bcd6b29fba2bc5a30b777f7cbba8.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: bark_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36455_44086133_jpg.rf.185f96367b51d06061fa8eb68cef8d84.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_74670700_jpg.rf.62b20fe479a2e8de7bbf659d83d09a54.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_75081455_jpg.rf.d033b460bf5de8ccfb7c899e0a10d9ca.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_75082114_jpg.rf.aadc40f2fcb49e8ecd3b162cac924f9d.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_75085176_jpg.rf.924a45b92782c9b4fe4bdd38cda296af.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_75312788_jpg.rf.ae5484631c4fb61034bd1cdfc57fe1e0.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_75331135_jpg.rf.fc4b9c942fc71d4a5cfa2a18f609ef34.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_75379666_jpg.rf.74b5f91901de586471ed7fea6a632af8.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_75432775_jpg.rf.5c7e946a84104ae43bc10b63191ac873.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_75501234_jpg.rf.37f2356ce6f18ab952fd8e9634ae722e.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_76571017_jpg.rf.7801f35cc444ceb4ebd0a0b4ea074a7a.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_76820250_jpg.rf.f3b65f1f15d6da79c38665fe08b1bce7.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_76946064_jpg.rf.6dfa61a5ed8ff125784d79cbb589822e.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_77401403_jpg.rf.8eef09b1a0fb9e0b60119488137c05e2.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_77401649_jpg.rf.5d6837f096f640c704adfcd929611984.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_7747514_jpg.rf.7bd4dd5a16c146641428568925f1ae43.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_77495682_jpg.rf.205b190ea49a8dbc3a796cccf9cc189d.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_77515687_jpg.rf.bfe934c8f2eec6771c12dc7ef40039cb.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_77585520_jpg.rf.dbe4c1ff2a888844b8be6536f9d9a05b.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_77642871_jpg.rf.6fa192c29989249c2af34e70d8832042.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_77649208_jpg.rf.0606312407eeaa9abd0bfe2f5f637103.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_77658863_jpg.rf.c994965ddbf81383dd02f6756cff5e97.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_77742697_jpg.rf.f061f00d07ae53680c3d1eb4bece510b.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_77818953_jpg.rf.86a48cc552716668d414a8c89e738bc5.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_77828280_jpg.rf.cdaef171a9d87786697fdae708806dd8.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_77845172_jpg.rf.1b4dc4e4825c7fb5181e4fb0a3c8bbe8.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_77897248_jpg.rf.ce48e0243a19b367324691b403e1aa0f.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_77934504_jpg.rf.35cc808f113ab3bd8af680e48e26c214.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_78091072_jpg.rf.a79dde4275a4ebcea3be213fed1e397b.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_78307708_jpg.rf.f11560bd59a86be908561b26229e6db1.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_78391712_jpg.rf.bfe03b3b62e9958c4a236dd94c5fd65f.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_78471661_jpg.rf.56c9902bd398259a39b3600b6e547c28.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_78481843_jpg.rf.39306f07addcc3801d8b39cf952db0a2.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_78499025_jpg.rf.50aa086b7a5ce6c9f557bf352b4c140a.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_78526825_jpg.rf.25a4aefa9c02b9b4ee979fcc46c4f213.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_7908707_jpg.rf.c488f98737722f36a1b5030e8a412e47.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_79273866_jpg.rf.4b738b4baa5b457c51862986e0e4d87d.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_7943313_jpg.rf.1c85fbd02a599b9c0ec8c2fa6bdff218.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_79644403_jpg.rf.4e1392fc61ff94e4738f86c6a5204ef8.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_80067186_jpg.rf.060e05302daaf806badd9b7b3f9022ee.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_80087247_jpg.rf.29226ca961937b7daba335c5a4a8388d.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_8021501_jpg.rf.1ab15b7e99ef7bdf32fff2f42c018771.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_8021502_jpg.rf.2e3b028b487007a6f238bd9c71a8b9d1.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_8021625_jpg.rf.ae6cc8867bb1a3cf42f4b0399d481382.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_80369854_jpg.rf.a61aca5d8bcd4d4e50c078b217db8689.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_80371733_jpg.rf.da20c99ec7e1ba89b12e3580f551489d.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_8045710_jpg.rf.7c6127b4f362bd354ce12505efbd8e31.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_8045731_jpg.rf.0aafb4eb96a168da3e2f39f969d2bb24.jpg: 640x640 1 lizard, 2.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_8045765_jpg.rf.f48dced5909b91efb69693fde6b77588.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_8045919_jpg.rf.e9be50a43a32dfa736424cca1c986bfe.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_81805618_jpg.rf.bdeadca18d48b520ed0574b55a9716c3.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_81895178_jpg.rf.3c072d49a0fb28de0a9e707df7a66a87.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_8247169_jpg.rf.c8b4e0a689b53d32fbbcd9f0ec34a97e.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_82475316_jpg.rf.aae991a7af0061e21e9d3a9dde22cb12.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_82622884_jpg.rf.0c0acd5618c0956297464362454b9b4b.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_82921413_jpg.rf.6394ec935c91a154387a6ccd16403095.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_83032890_jpg.rf.831341bb84376c1c32797bb5a784bc53.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_8334980_jpg.rf.d80413f88ae7240da430fe370e79136e.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_83567606_jpg.rf.5edfb20034477d4063b492898e387c67.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_83705008_jpg.rf.a3e25a6d95da84bf1ead018b28f8a9d9.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_83707370_jpg.rf.d2e125241e8e6ad2060931c60a04e663.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_83742553_jpg.rf.bbf098dee4fc0bad025f6031fa3cc4b6.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_83962764_jpg.rf.36794698885780654fc108ffac3f54e9.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_8400984_jpg.rf.a7375f1f2181504bb2687192077d38a3.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_8401017_jpg.rf.968ff7917549499136cbacea94e00bb4.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_8401056_jpg.rf.5ceca88170dedc35b847dcb4fbb473ff.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_8401074_jpg.rf.21f0e382c8b565c871b0304c7b01a899.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_8401206_jpg.rf.3c6dcd00fdd26a424ff2b640ae298152.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_8401729_jpg.rf.343c1eb5db77d31223e6b4ac9a9541df.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_84202654_jpg.rf.69837180781cd4772cac63511d6e151b.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_8476571_jpg.rf.2411a613248dc5eb1b29034541cd7b25.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_84838868_jpg.rf.7e7f7e069756f1e873a3afa752ca9f38.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_85053705_jpg.rf.e7f1324be96ecdbd84dc9868282428af.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_85059967_jpg.rf.160fdaea6d3ba921e6d95c307d061d22.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_85134369_jpg.rf.d64cd2d4caff6ea56abf16a25ac12e69.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_85289003_jpg.rf.d5bf80d72177c0b07b021844efa7f885.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_85483047_jpg.rf.0e38144b8d6c3a9a7a3d683e49acce2c.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_85701612_jpg.rf.5726456dd874bef7c691d81930483802.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_8574831_jpg.rf.93b419b0339c41614817e72a078c9e4b.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_85931117_jpg.rf.8aa06564f4a38ddc9422d5900a0ebd1a.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_85943023_jpg.rf.554809133bae33dd7a393760f9477cda.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_85982595_jpg.rf.e909d21cb2917786bbca6d86870aafd3.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_86037862_jpg.rf.b0b593878345ec37b2028ce4f9eb7479.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_86208334_jpg.rf.ff0f9a7f12b23936c80e0055b3b59ba5.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_86208956_jpg.rf.d3b5854aeaeabc6e2b07dea4a0044ee0.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_86279962_jpg.rf.d28a3de3c3878b4649c290e55d569918.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_86462392_jpg.rf.9ef756c36f5a3ea39453a61ef6317edc.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_86489350_jpg.rf.ec899f2b5b56fcfcc038a7e84c85b9ca.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_86604642_jpg.rf.e81be5a48048e70cf356542778997b6d.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_86898482_jpg.rf.8c315656015f181f57343fe5ef3c20a4.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_86981664_jpg.rf.8859f00ded210bf4f2d3fe06c8e63488.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_87319567_jpg.rf.e871ff8fba922d6030c17a74e16919ec.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_87452269_jpg.rf.506048bde133186a580986316fc14b25.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_87569192_jpg.rf.1d81ddeb16139a6b56f9df425038b9fe.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_87627436_jpg.rf.6fc4b16922ade72aef6b07359c38ea25.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_87916795_jpg.rf.0d1270013d8c47600332ecd392b2e15e.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_901021_jpg.rf.a05d18a63475c96eb27f56e5c633a201.jpg: 640x640 1 lizard, 2.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_90345468_jpg.rf.5cfedb90ebdcd4f1d76740815563feaa.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_90382167_jpg.rf.9bac0d30cebe4c6c17d76321513b4c3f.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_90424372_jpg.rf.f1f36aa146fe572cd07e4e8953fdf6b3.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_90707272_jpg.rf.291ae9849de81093d7954e8de2eb44ca.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_91329996_jpg.rf.b3065add46e04b777facc160e8999e5d.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_91495801_jpg.rf.b46c94ad75f2a5254e67beef1ac90752.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_91597736_jpg.rf.a58bb496e1b77990d9ca59d9b6c1a85d.jpg: 640x640 2 lizards, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_91857315_jpg.rf.fdfbbd20e393a50230f0603d33eb5f56.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_919013_jpg.rf.c1abbd3f71f4b75e0959af36b0567e00.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_91959201_jpg.rf.65a182c7c0c377085061f239899644e1.jpg: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_91979529_jpg.rf.a3dea6439e2f619d2cd7049879df8744.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_92195440_jpg.rf.095ae6c3e16c1b5dbfdc93bc2b94bf90.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_92514623_jpg.rf.f5c51c7605e39dd5385ab080602fdd1b.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_93152758_jpg.rf.a1eef01be626c4c5aa4aba79581f7b38.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_93281885_jpg.rf.d70ff4df95df97f0103024c50ffde4bf.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_93397135_jpg.rf.30f21c3873cbd90c20c66b0313037bb2.jpg: 640x640 2 lizards, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9340760_jpg.rf.98aa63c4e11ebb157aecb7dfca003d4b.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_93670493_jpg.rf.48b635ec558219d41c4f514b0195a1cf.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_93828622_jpg.rf.946028e21bf5539ddf1ec07d06e9c63d.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_93883314_jpg.rf.b3667d22f04e342bf63a59a5f18712a5.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_94009183_jpg.rf.894e73e1202f95d8d48ee6145ade19e2.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9417986_jpg.rf.d8fb416378e896d2d3c8f2111f22e09b.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_94262444_jpg.rf.294b96306d4f8999bdec903dc166ff0d.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_94292332_jpg.rf.3c635d74567ceeadc77a775ed4593ea1.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_94579330_jpg.rf.61f828c0aa8ffe7aa161a9f0d3f90f5e.jpg: 640x640 1 lizard, 2.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_94695765_jpg.rf.9c44c6cd01bd536458b7881ddc890c32.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_94899312_jpg.rf.a2804780deeb77feef25499318683a08.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_95147733_jpg.rf.c6016bf28aa3276aadb87e0f7d77e514.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_95212581_jpg.rf.3673d75e8808f660bb8bd49c88eb251f.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_95317737_jpg.rf.7e38950287848f5dc479f3659e45e688.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_95430330_jpg.rf.0645af1b621e0372a7332674ff0b6412.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9562622_jpg.rf.13796bdc5abd9f4aaf8a9a01a61596eb.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9567266_jpg.rf.49e9abafaecdc1e75e60254913652b2f.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_95712332_jpg.rf.7ff63685548a94c19ab11289d579366a.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_95813687_jpg.rf.b5ddcae91dd9f411719e84cb07ddc27d.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_95814676_jpg.rf.115313473eb49c3da2047e96bb25a345.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_95815144_jpg.rf.5e69d8d68f6cb3906f9c26015e7619d1.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_95815323_jpg.rf.6fcbff7514de2e5482f002d133728863.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_95815359_jpg.rf.6236021f4db16294bba0678faa9296b3.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_95815604_jpg.rf.a5287cfaaf42151296fa3165b076bd5d.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_95815641_jpg.rf.1b68a1537994dd6f82a673b9c665862e.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_95815670_jpg.rf.926251ac03b456c3cdb3967d89424fc0.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9596277_jpg.rf.8d690ff73211dcb773c29333833649b8.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9621392_jpg.rf.4cc192bdbf25a41157fb3e15f5cf35f4.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_96306271_jpg.rf.e62a08b7068ac55aebf394061b8e1207.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_96323006_jpg.rf.afd71f6631cc9156d73c147c789efe6c.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_96323009_jpg.rf.448f3ca49ab3740ad1ccf376b2b6990d.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9640890_jpg.rf.6a1413f59ad82fdbb9ae384490ee7f3f.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_96415928_jpg.rf.a0ecf85a0ad7a2ec6f0a72363891da90.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_96674004_jpg.rf.160cd4fae4468fc0bdac844c445042f9.jpg: 640x640 1 lizard, 2.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9746062_jpg.rf.b44da045c5a0d1646edbce82ead245ac.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9747056_jpg.rf.da7649fbc8799a5e666f5aec2727ecc0.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9750330_jpg.rf.12aa37129412399aa2caa04b04d89d2f.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9756538_jpg.rf.fd2fa8070d358adcadb08713608725b3.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9797463_jpg.rf.7aedf8063e1254c5716d7d6eb19d56da.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9811279_jpg.rf.e17e502b5be3c45f18c842bfae7dc121.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9815004_jpg.rf.3dceec6e94bc8f3b9b2acbb060e94d94.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_98595254_jpg.rf.a523220cb430eb33ea08c1e02f835ed5.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_98718766_jpg.rf.694d2161e9f85a2775d82d79fefbb1e3.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_98739785_jpg.rf.ba34c27f3b4a014e6b904b5376d08531.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_98830448_jpg.rf.7a9d4bb1ab2cda953d7bbffcd6e019fe.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9893312_jpg.rf.507d63593cb568e91126dfb30d8924c9.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9894586_jpg.rf.b7693fc1fae4b5b94cd188118fca207d.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9909761_jpg.rf.7e21776cb0a243483c29b179906a665e.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_99099360_jpg.rf.f322651f5712592c78acff4a003c6480.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9910191_jpg.rf.ebf19278172d3cda27d8704005f9ebc0.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9910262_jpg.rf.3dda21ca8fb2b486089fd2c467fd7004.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9919458_jpg.rf.f95054a9153f5c9069541ecca8355aa8.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9921265_jpg.rf.5e40e6f1fe7d068605848b4e5bba70a3.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9921304_jpg.rf.9d8a3fe81c612619666d6bc252467bc3.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9922113_jpg.rf.26ebdd05ad704061d19ea168ca1f8879.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9926949_jpg.rf.b3df198368bdd2496a6419ccb1843a54.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9927238_jpg.rf.ccf06bbf1128795424e1db645a80952f.jpg: 640x640 2 lizards, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9927309_jpg.rf.650b5cb7335cfb7fc8ff50347545389b.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9928808_jpg.rf.0012e7de66f9b2204700017b726668bd.jpg: 640x640 1 lizard, 2.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9931139_jpg.rf.0b738cb75cb5f1739c05561d8ec4d054.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9931168_jpg.rf.35f500da26c89115a0a654aa7b84af06.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9935090_jpg.rf.0928141cf61f08fbaab7c90242a60c5a.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9935451_jpg.rf.c703d65734db5073de327a4658f9815d.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9935667_jpg.rf.2570f5649046364077419990721bc309.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9935900_jpg.rf.c30cf71e743e9bc5abb930a6ad52bbfb.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9937770_jpg.rf.3725464e49015f673ff2ed1d17f5e3ab.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9937812_jpg.rf.342acbe122a23a4ec4d68cf57626ee1b.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9937856_jpg.rf.a7488ed8ad4eae39d9601a41d21b3614.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9937888_jpg.rf.4feeddf9d9eab44f7acf2c478a57faaa.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9937893_jpg.rf.12492a609517caa7a8e2683f3ec09d1c.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9937970_jpg.rf.944f124e6afb59f954a9d6be59f49d08.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9937998_jpg.rf.1a783c9ebbc311a558bffab5a3188da3.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9938283_jpg.rf.16ca5754a9bf0fed84b3d8fbefe0a4be.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9938297_jpg.rf.9c878e0da4d270978bd41d6921a48052.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9938436_jpg.rf.aaa70229183d157a2c43e3727b9d3fef.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9938503_jpg.rf.b77a55de4585b4b6ef62ab7fdf2f642a.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9938594_jpg.rf.e175c3d68bf8f2cb1d112d9f6fb1b6a7.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9940372_jpg.rf.9a726171b8994f7a90c45a224fcfce2f.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9940597_jpg.rf.e6509d4b0970e9b4e574d7b4526cc388.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9941006_jpg.rf.d8c9a8efb1787d831ddba572875cd1fa.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9941039_jpg.rf.ecf26f34e2d915474071a43c944cef1d.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9945959_jpg.rf.4a171ed4476445d0649953471bce8cee.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_9950192_jpg.rf.e136fbd373a9ffbdd403c9ea589b7035.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class label: crested_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36488_99896177_jpg.rf.305624cc93bdde8747fa6be4714e0ae2.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4203810_jpg.rf.d4ed08476c3e911ebe9acd588115a2c4.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4204312_jpg.rf.594882bb98f5aa0b75e9a103a1578634.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4207229_jpg.rf.efdc1b5fc97b03581733d30456089b66.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4207772_jpg.rf.2eaee2e6b6c3e90a35ad44c772abc67f.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4211023_jpg.rf.23b799c654ea3b0b6529da5d0d4f63bf.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_421208_jpg.rf.0e5d44c9dde653b606fe704a6e750968.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_421350_jpg.rf.b2be05e21b32ee6cf50a370b20ebebb7.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_421597_jpg.rf.d2c0d66f73f53b9966bba9a249decdef.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4218750_jpg.rf.cff76ff304da1fcec246b1cce1ea0f35.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_422197_jpg.rf.79e9d99d1057f1859fe4f6c6620ac006.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4224527_jpg.rf.bf4407dc888d81e81232ec6a2cfc78bc.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4226702_jpg.rf.5e58b932991aa927ad2f82c510c788af.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4226934_jpg.rf.faedf050568afcebdea5b4d60fc39097.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4227674_jpg.rf.1faa1c1a942639bdcd39865edee980cc.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4227932_jpg.rf.29d08e84fdd09bbde406bff24075e981.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4228220_jpg.rf.f3d06b9a7eeba095254452b87b4a951d.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4231203_jpg.rf.278e9800a4fac24c0fc52bb1c1162902.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4231206_jpg.rf.5012b98804c1f26941a8bf14c6b1bec7.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4233739_jpg.rf.0fffe3246727ebc8c9080dcdbbde3e0e.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4237588_jpg.rf.e24c630b8b5a88dc1cb4b9b9896ed388.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4241178_jpg.rf.619a45f504807ca9db6c901abc624cbf.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4241810_jpg.rf.22c8a189a54580811445865b9ddb0076.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4243807_jpg.rf.e2df36803b681bcf5577b07dfda35517.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4245819_jpg.rf.386173c281543ca47c5b762f4f6a77f4.jpg: 640x640 1 lizard, 2.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4248904_jpg.rf.cada590629b3e54181f54627475d9103.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4249241_jpg.rf.ba211ae64867fbd7d89445fb4a3877f2.jpg: 640x640 2 lizards, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4250193_jpg.rf.0da5da3a4e74dc37d1eb0aa0557a99a0.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4250660_jpg.rf.ee8f0b5156fc405d65b28a4f84bb7de8.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_425157_jpg.rf.a294a3a2ea7c6aac781605ea0f2fe515.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4256063_jpg.rf.46f0eb439e03b7b4b22d7c6d4b6e5dc0.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4257657_jpg.rf.e863b1c40ffce84140ef512718576cac.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4257671_jpg.rf.2bb0812b95c3daad653d2357933404c8.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4258984_jpg.rf.f66338f8b2ae567e61d68ab94fda7186.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4260558_jpg.rf.3497be122bc1014c15665e173be42f8d.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4262305_jpg.rf.f14726ea2bb1e9fa328ed12252dad2ba.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4265734_jpg.rf.6fdce92a924e43d1871bb9642ae17ee5.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4265853_jpg.rf.33747104e105a89b2e50c93d036d6be6.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4266460_jpg.rf.f5f25171db1214662e7f36d900b18c3a.jpg: 640x640 3 lizards, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4269115_jpg.rf.8ed45efda317f1ef4a271ecb5a7609f7.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4269777_jpg.rf.6e3fb24568bf367b065adfc04bc3fbb3.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4271867_jpg.rf.0efc4b543904b033a1200c264f4b7f80.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4278137_jpg.rf.5b945abb4666f7eb4e4f1526e6626c86.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4278475_jpg.rf.2b8660a8f3bee2d5b096c465bcc7099e.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4286174_jpg.rf.905af42a772c69b1dec24ac377887c22.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4286610_jpg.rf.3e84dbbe507d3eba717f88019b36d2a8.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4290923_jpg.rf.2d9308986c2e8f4399dd7d9542c967f9.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4292151_jpg.rf.e22da5bc9b4ee3d0c301a89d91ed1828.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4292959_jpg.rf.810a9ed25a6b79e4c67d436464cdd78a.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4294106_jpg.rf.df0b2e2e6563b486d23a97c8525ddf7b.jpg: 640x640 1 lizard, 2.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4294891_jpg.rf.d7cf370adcb5079cb43440b17a9fb814.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4300539_jpg.rf.8c7df0ab5e09749237b115ba2eba18f4.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4300903_jpg.rf.170d3b6f4aa3a88fad49bf8bf6c362ff.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4304794_jpg.rf.b7612763f65fa497643eacd1f43ba27b.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4305065_jpg.rf.97c5d1253861c7e8df5540f3dd22e954.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4305182_jpg.rf.ee584623f13b78a8770adf4e0408d5d4.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4306887_jpg.rf.4a50b0975926b97a4527be65b3daca54.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4308311_jpg.rf.b1df36b58e6ae46d210058202ea8d055.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4310294_jpg.rf.4db26d3bbc96bf92b81db5d8af2764dd.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4313803_jpg.rf.a50e8b47a54e05fa7ca0c835bd673a4c.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4314321_jpg.rf.fb9f7af745ab3ece8734869485aaad3d.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4314336_jpg.rf.fbb6328f6961b70b6ac23088e19447be.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4315094_jpg.rf.4c5d49e8cfef5bf4ec588771d7bd5196.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4315255_jpg.rf.79383311e4d08842f139490bcfce4237.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4316757_jpg.rf.47312b3471f623cd2869e58495a7c971.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4317586_jpg.rf.a3edae6beb67809c37e52a13d87ecfbc.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_431858_jpg.rf.79815d289799b0e52c3d2c64691378cb.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4319646_jpg.rf.f8dfcb92d108c75436efb97e9ee87963.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4329662_jpg.rf.e37692a37115398555991d9ec53d94a3.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4330095_jpg.rf.29a98c7ca2e9999e152cd41e2fe9a2fd.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4332657_jpg.rf.1050543fbba5b74c8225ce82d029c70c.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4332811_jpg.rf.862cad811ff322c4c03bea85f91dc45b.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4332851_jpg.rf.02a4792ba4fd4406816c87dfbcdb54c9.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4333898_jpg.rf.a31c2fa01941a1bc4455c6f9afc45fec.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4334713_jpg.rf.7fdcee84f4878b2b79d35496e58205cd.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4336233_jpg.rf.3a2ca4f0af9d4a8237148d522b5b7650.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4336386_jpg.rf.8299673b5af7cb698e2f8c6d05431023.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4340402_jpg.rf.42d04dfc7223f0f8b3b36a8e8342663f.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4340404_jpg.rf.7cc413c9235c8cd1ee732f40fd3bbff5.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4342455_jpg.rf.36a0b0b280966a9e812d6b5e9d81545c.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4342823_jpg.rf.c721d21666df7f47cb04ee6325f7f083.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4342981_jpg.rf.dc61508a6e6d1a119880e6c6af2b5535.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4343051_jpg.rf.8eea6c3d1b56e8c4c93f55a1edd0227c.jpg: 640x640 3 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4343056_jpg.rf.7e7d865487c5b49e656fa4e67fc801b9.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4344347_jpg.rf.05151393747cf05ff8d00f3636a3eb6b.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4345064_jpg.rf.66aa82a71567f8a6f7bc9afa24eadeb3.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4347288_jpg.rf.ff8fa21b98484bf5338e7835ae9e5f98.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4347824_jpg.rf.7e22c58c65150f73ab51f94eefe62fff.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4348390_jpg.rf.828a9decdc49000f59e6de4fef7651b1.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4362117_jpg.rf.4b19f25dc146b5ef27c76d40a1381e20.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4362422_jpg.rf.24f3c2f4877529913ee71d779cb80efb.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4363248_jpg.rf.28ba608d36f29ef007bdcd99264d3a55.jpg: 640x640 1 lizard, 2.2ms\n",
      "Speed: 0.7ms preprocess, 2.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4363848_jpg.rf.dce0cd7235ef55374b62c64abff1d2e3.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4363920_jpg.rf.6c31804a91877ed7f1e6e2b42de7b962.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4364284_jpg.rf.e0f032c1ad5a9626234f2723ac336419.jpg: 640x640 2 lizards, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4365752_jpg.rf.a24c4a91ee5614b8ffd96c6cc25e1b3e.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4366133_jpg.rf.f8aa5baaf12e95163e5c5080abf13fee.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4366910_jpg.rf.c8097f7571565aba2de1f7100e9c8fd2.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4366982_jpg.rf.8e27eb8c4b4ef010ecffacd87b627548.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4369957_jpg.rf.fe7aa1f3c2ae07c775f3e6280603fdc5.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4370382_jpg.rf.a117e370d8b3136c4df632b644c8039a.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4370726_jpg.rf.30431ac57105604af7a68a2ce4c9758e.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4371443_jpg.rf.0e39d53d5972573f1175fb1babe94b2b.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4371612_jpg.rf.8d92231dfe9f36b616231ab09931f7c9.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4372287_jpg.rf.a12fdb6c6130cefc7e909f6f1b3a48ca.jpg: 640x640 2 lizards, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4374807_jpg.rf.69d25f34e1ced97d7296c004862df0e6.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4375248_jpg.rf.9852b228a9994e75ffe16eeb2f3b1d69.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4375299_jpg.rf.34ab5f8d1705e42914d24c85cc8f6d07.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4375497_jpg.rf.3c4449531abd27625e797d23aaf2cccb.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4375970_jpg.rf.96490706ecc7f04e4545d2186a7ad0ac.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4379209_jpg.rf.2efa5c16c630dafa540c341c64b97246.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4379551_jpg.rf.c3b6f1967cb7669f42b38e3b84d862bf.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4380022_jpg.rf.7cac445c6e05ece4e594c2f2bce67152.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4380581_jpg.rf.a2f4667cd009e791c2f94089cbccc8bb.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4380999_jpg.rf.0240c487bbe8e02d5651ba08433a70cd.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4381640_jpg.rf.552aa78d9994c9ec7c3e65fb64f33c33.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4381714_jpg.rf.b75d420becebc19ff8e1ad38de0c31bb.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4381826_jpg.rf.8c3ded20d393d8343faa6fc9e11d610b.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4381977_jpg.rf.91223a76606a895399bc81a42f6c0b64.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4381982_jpg.rf.5b27238cf557c3aa6ecc781e4b185c68.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4382934_jpg.rf.5ee90896a49ba017a15031bb80c0f8ff.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4384716_jpg.rf.ba6de19c39d92b5c3c6ae8b3a8471864.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4385564_jpg.rf.be71d96f3ec89ac4702959afdab67d5f.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4386025_jpg.rf.1cc5e5cc3b3abc94d4b6617436db6f32.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4386395_jpg.rf.04b9cb0591dc87542ecac23f4a32d914.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4388342_jpg.rf.cf565b6b7948d4160966ef13849cf37d.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4388757_jpg.rf.143db4378140b11019efe3f5fdf7b775.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4389006_jpg.rf.d08ce18a28faa699608eb42bf7213097.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4389492_jpg.rf.42cd6a39c8d4c493dfbecf694d1ee17d.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4389518_jpg.rf.b0e775e96bd5cb91d2517d0c7263f5a2.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4390116_jpg.rf.f66efa139d878e8e29f99977f02c7ad5.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4390554_jpg.rf.33d5ec553a34f7e8d3a646de30cbbfef.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4393527_jpg.rf.6a151aa67815563a2c01c23d315bde48.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4394033_jpg.rf.e2f9fdf1ecc5624113a115ab16ab8b2c.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4394036_jpg.rf.39169c60ae558e0894797bf6d01a18d8.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4394160_jpg.rf.19d112cf08ab7259e69fa543e80965df.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4394525_jpg.rf.8c1f52d34e9397afb53445e222e53c74.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4394910_jpg.rf.9700ad04d5c9b2309997ddb149bde2f2.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4397099_jpg.rf.8cc8be0cd47269279dc2d881a581a614.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4398377_jpg.rf.e8d6994dc2b7a61bc2d54648c101f940.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4401097_jpg.rf.50933da527986e6a90439ee77075325c.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4401157_jpg.rf.77b7c28061e930b74f20a05b4bcb3f8b.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_440189_jpg.rf.cbbcc4ba62f87c421f2790699e2b70ac.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_4404034_jpg.rf.98e7c75dfa866aa206e0bc837e4e0635.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_441179_jpg.rf.27502e859d906494b995705728b825ae.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_442047_jpg.rf.641e0ea74ff951608488957fef9ea4f5.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_44482_jpg.rf.12b6e5a3436416a484fbea28ceb1d131.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_445333_jpg.rf.87e8fbb8804d35f0d41f5fe4bef45fd0.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_447315_jpg.rf.32271fc1341391acea0cc313c111c618.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_449785_jpg.rf.940da124f14ba79472324e6032d40189.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_451605_jpg.rf.6fb67c2b626e3215af255570e8c4d508.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_454632_jpg.rf.4b96b66d9eb029dde31a1554e1814810.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_454647_jpg.rf.2b1c8dc3087535b957cbdb2361ff093d.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_459273_jpg.rf.3a03fa7feac148c4eeb2ac1d41cc52e2.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_459487_jpg.rf.9af9327608144a8139a34db5763cc274.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_460586_jpg.rf.47a81d7e2de6047b31f63bf9a669a82c.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_460604_jpg.rf.10616a7dbed4c74ff17e5e34267c33d4.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_462497_jpg.rf.29fc07e27da448503d2723eecb6b0585.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_46249_jpg.rf.0f97923125ec4b3a99d5bdb608319804.jpg: 640x640 2 lizards, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_463574_jpg.rf.3d8f78c44aee39a246f8a9e3279f4738.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_463592_jpg.rf.075df55cbfa0a98b65fe1a70defaf02c.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_473877_jpg.rf.f10e1622f82b686550084d3b5f389964.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_476159_jpg.rf.cb7a4c43a0d443de2c5ffe4ca78ccaa2.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_485276_jpg.rf.65a474548da11559d6a35de231560f1f.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_493595_jpg.rf.27a63766000f73c4e118cda84fb440b1.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_493597_jpg.rf.a8c878da14216680623b9fb05b59e36f.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_49465_jpg.rf.f98da8e3d78df87893408222fb4a8998.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_496417_jpg.rf.0885913cb155c61c9efbb83e077aa1a2.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_496436_jpg.rf.f86f12b5ed181145fdea83cbfa10a11e.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_496493_jpg.rf.3a47b1abf47eda3e179f7365d340bed3.jpg: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_499249_jpg.rf.bb1bc395d7dcb07c7034052e64be178e.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_499811_jpg.rf.27ecb9c7ce9fc988fe653cf5a8c98d98.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_499821_jpg.rf.addb5a25ebee89ba82d82082576d2479.jpg: 640x640 1 lizard, 2.6ms\n",
      "Speed: 0.8ms preprocess, 2.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_499871_jpg.rf.051cffcf40a50a8950c47068e10a3db5.jpg: 640x640 1 lizard, 2.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_499894_jpg.rf.fad4b796566d05dc925d4f18081c3ea6.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_499960_jpg.rf.93f4cd1e230b379bf6f29a9f2878dc71.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_500644_jpg.rf.eb7099ed6815318ce3479c7a9ead650d.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_502234_jpg.rf.3a7300029cce5e47982ba040696761e3.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_502385_jpg.rf.38fb4123ee145ef9ace2d499cbf18a44.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_502973_jpg.rf.00dd8130359befdb10d41b4b7bfde7a8.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_503432_jpg.rf.59d3d7b7576d0ee5ed3b10d4c59586af.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_503466_jpg.rf.b2a5f1524a1fd390a41f252202803bfb.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_503482_jpg.rf.404871063ec4c30833b9de874f238a9a.jpg: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_503484_jpg.rf.9e6f5dbf2e9b9d1158d872d7a47547ab.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_503487_jpg.rf.04cb1ca6bfa625c4986461d7771354b3.jpg: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_503489_jpg.rf.a7ad9f71c0410eb44f53d965eb926d23.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_503492_jpg.rf.eac8c1e6ca654f14c56008a0670ce337.jpg: 640x640 (no detections), 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_503500_jpg.rf.20e53f1f21cc5528561089669ec3cbe5.jpg: 640x640 (no detections), 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_503502_jpg.rf.475fd40de2e557858841d237f45ff767.jpg: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_50394_jpg.rf.11996722dc5edd7d4bb49516db18b8fc.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_504825_jpg.rf.5f6fc65f633dc8d468685a66f90b0130.jpg: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_504827_jpg.rf.4d2b5cf4d04578f79c5a2652f1ffdd69.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_505559_jpg.rf.fafc781974f617ab8e9daabaa4efc5e6.jpg: 640x640 (no detections), 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "missed detection bb confidence: tensor([], device='cuda:0')\n",
      "\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_506491_jpg.rf.2848bbce84f7a65b78fdd5657277e1bb.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_506500_jpg.rf.71f7220881670a2e89c2d4a67cab4110.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_51320_jpg.rf.b09b2fd6ebc5a3925a8ce4adccb2f7b4.jpg: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "class label: green_anole\n",
      "\n",
      "image 1/1 /storage/ice1/6/5/wchia7/Anole_classifier/Spring_2025/../Dataset/yolo_training/florida_five_anole_10000/test/images/36514_51431_jpg.rf.86be6d484bc2f12be1652974e3b70e9a.jpg: 640x640 2 lizards, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Missed Detections: \n",
      " {'bark_anole': 15, 'brown_anole': 11, 'crested_anole': 7, 'green_anole': 9, 'knight_anole': 14}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pipeline_eval import OD_inference\n",
    "\n",
    "ODmodel_path = \"./runs/detect/train_yolov8n_v2/weights/best.pt\"\n",
    "\n",
    "test_folder_path = \"../Dataset/yolo_training/florida_five_anole_10000/test\"\n",
    "dest_folder_path = \"../Dataset/yolo_training/inference\"\n",
    "\n",
    "OD_inference(ODmodel_path, test_folder_path, dest_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cropping Image\n",
    "Will utilize cropping API to crop images in folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping Source: ../Dataset/yolo_training/inference/run_20250503_023554/lizard_detection/bark_anole. Saving to ../Dataset/yolo_training/inference/run_20250503_023554/cropped_image/bark_anole\n",
      "\n",
      "Cropped image count: 181\n",
      "Images successfully cropped, resized and saved to ../Dataset/yolo_training/inference/run_20250503_023554/cropped_image/bark_anole\n",
      "Cropping Source: ../Dataset/yolo_training/inference/run_20250503_023554/lizard_detection/brown_anole. Saving to ../Dataset/yolo_training/inference/run_20250503_023554/cropped_image/brown_anole\n",
      "\n",
      "Cropped image count: 186\n",
      "Images successfully cropped, resized and saved to ../Dataset/yolo_training/inference/run_20250503_023554/cropped_image/brown_anole\n",
      "Cropping Source: ../Dataset/yolo_training/inference/run_20250503_023554/lizard_detection/crested_anole. Saving to ../Dataset/yolo_training/inference/run_20250503_023554/cropped_image/crested_anole\n",
      "\n",
      "Cropped image count: 189\n",
      "Images successfully cropped, resized and saved to ../Dataset/yolo_training/inference/run_20250503_023554/cropped_image/crested_anole\n",
      "Cropping Source: ../Dataset/yolo_training/inference/run_20250503_023554/lizard_detection/green_anole. Saving to ../Dataset/yolo_training/inference/run_20250503_023554/cropped_image/green_anole\n",
      "\n",
      "Cropped image count: 187\n",
      "Images successfully cropped, resized and saved to ../Dataset/yolo_training/inference/run_20250503_023554/cropped_image/green_anole\n",
      "Cropping Source: ../Dataset/yolo_training/inference/run_20250503_023554/lizard_detection/knight_anole. Saving to ../Dataset/yolo_training/inference/run_20250503_023554/cropped_image/knight_anole\n",
      "\n",
      "Cropped image count: 184\n",
      "Images successfully cropped, resized and saved to ../Dataset/yolo_training/inference/run_20250503_023554/cropped_image/knight_anole\n"
     ]
    }
   ],
   "source": [
    "from pipeline_eval import crop_image_individual_anole\n",
    "\n",
    "src_folder_path = \"../Dataset/yolo_training/inference/run_20250503_023554/lizard_detection\"\n",
    "dest_folder_path = \"../Dataset/yolo_training/inference/run_20250503_023554/cropped_image\"\n",
    "resize = (384, 384)\n",
    "coord_type = \"xyxy\"\n",
    "\n",
    "crop_image_individual_anole(src_folder_path, dest_folder_path, resize, coord_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference from Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-classification\", \"swin-base-patch4-window12-384-finetuned-lizard-class-swin-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d195d247ca43a99c80cb7883d1cede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769d36fa9ec04aad85135b5451644c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/927 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1ce0a3012540f8a7f9ff0257c8d6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cropped_dataset = load_dataset(\"../Dataset/yolo_training/inference/run_20250503_023554/cropped_image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = cropped_dataset[\"train\"] #Have to get from train b/c there is only 1 folder in cropped image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image = image[\"image\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'bark_anole', 'score': 0.9995412826538086},\n",
       " {'label': 'crested_anole', 'score': 0.00044674609671346843},\n",
       " {'label': 'brown_anole', 'score': 7.2053217081702314e-06},\n",
       " {'label': 'green_anole', 'score': 3.1463248433283297e-06},\n",
       " {'label': 'knight_anole', 'score': 1.8368094742982066e-06}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(target_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9374325782092773,\n",
       " 'total_time_in_seconds': 12.61808353709057,\n",
       " 'samples_per_second': 73.46599008281287,\n",
       " 'latency_in_seconds': 0.0136117406009607}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import evaluator\n",
    "\n",
    "task_evaluator = evaluator(\"image-classification\")\n",
    "\n",
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=test_dataset,\n",
    "    metric= \"accuracy\",\n",
    "    label_mapping=pipe.model.config.label2id\n",
    ")\n",
    "\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation with Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "# Get class names mapping\n",
    "label_names = test_dataset.features[\"label\"].names\n",
    "\n",
    "# Create a mapping from label names to indices\n",
    "label_to_idx = {name: idx for idx, name in enumerate(label_names)}\n",
    "\n",
    "# Prepare predictions and references\n",
    "def predict_image(image):\n",
    "    preds = pipe(image)\n",
    "    name = preds[0][\"label\"]\n",
    "    idx = label_to_idx[name]\n",
    "    return idx  # Get the top prediction\n",
    "\n",
    "# Get predicted labels\n",
    "predictions_int = [predict_image(item[\"image\"]) for item in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground truth labels\n",
    "references_int = [item[\"label\"] for item in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check predictions / ground truth labels\n",
    "references_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'confusion_matrix': array([[169,   1,   5,   1,   5],\n",
      "       [  2, 168,  13,   2,   1],\n",
      "       [  6,   7, 175,   1,   0],\n",
      "       [  0,   5,   1, 179,   2],\n",
      "       [  1,   2,   0,   3, 178]])}\n"
     ]
    }
   ],
   "source": [
    "import evaluate \n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = evaluate.load(\"confusion_matrix\")\n",
    "results = conf_matrix.compute(predictions=predictions_int, references=references_int)\n",
    "\n",
    "# Print confusion matrix results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute f1-score, precision and recall for each Anole class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: bark_anole\n",
      "  Precision: 0.9494\n",
      "  Recall: 0.9337\n",
      "  F1-score: 0.9415\n",
      "------------------------------\n",
      "Class: brown_anole\n",
      "  Precision: 0.9180\n",
      "  Recall: 0.9032\n",
      "  F1-score: 0.9106\n",
      "------------------------------\n",
      "Class: crested_anole\n",
      "  Precision: 0.9021\n",
      "  Recall: 0.9259\n",
      "  F1-score: 0.9138\n",
      "------------------------------\n",
      "Class: green_anole\n",
      "  Precision: 0.9624\n",
      "  Recall: 0.9572\n",
      "  F1-score: 0.9598\n",
      "------------------------------\n",
      "Class: knight_anole\n",
      "  Precision: 0.9570\n",
      "  Recall: 0.9674\n",
      "  F1-score: 0.9622\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Compute precision, recall, and F1-score\n",
    "metric = evaluate.combine([\"precision\", \"recall\", 'f1'])\n",
    "prf_results = metric.compute(predictions=predictions_int, references=references_int, average=None)  # No averaging, get per-class metrics\n",
    "\n",
    "# Print per-class precision, recall, and F1-score\n",
    "for i, class_name in enumerate(label_names):\n",
    "    print(f\"Class: {class_name}\")\n",
    "    print(f\"  Precision: {prf_results['precision'][i]:.4f}\")\n",
    "    print(f\"  Recall: {prf_results['recall'][i]:.4f}\")\n",
    "    print(f\"  F1-score: {prf_results['f1'][i]:.4f}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Confusion Matrix (Absolute Numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Convert to numpy array\n",
    "conf_matrix = np.array(results[\"confusion_matrix\"])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=test_dataset.features[\"label\"].names, yticklabels=test_dataset.features[\"label\"].names)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Confusion Matrix (Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Convert to numpy array\n",
    "conf_matrix = np.array(results[\"confusion_matrix\"])\n",
    "\n",
    "conf_matrix_normalized = conf_matrix.astype(\"float\") / conf_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Ensure NaNs (if any row sums to 0) are replaced with 0\n",
    "conf_matrix_normalized = np.nan_to_num(conf_matrix_normalized)\n",
    "\n",
    "# Plot the normalized confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    conf_matrix_normalized * 100,  # Convert to percentage\n",
    "    annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "    xticklabels=label_names, yticklabels=label_names\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run custom script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 lizard, 2.4ms\n",
      "Speed: 6.0ms preprocess, 2.4ms inference, 72.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 3.0ms\n",
      "Speed: 0.8ms preprocess, 3.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.9ms\n",
      "Speed: 0.9ms preprocess, 2.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 3.4ms\n",
      "Speed: 0.9ms preprocess, 3.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.9ms\n",
      "Speed: 0.8ms preprocess, 2.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 lizards, 2.9ms\n",
      "Speed: 1.4ms preprocess, 2.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.9ms\n",
      "Speed: 0.8ms preprocess, 2.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.9ms\n",
      "Speed: 1.3ms preprocess, 2.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 1.0ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.9ms\n",
      "Speed: 0.8ms preprocess, 2.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.9ms\n",
      "Speed: 1.0ms preprocess, 2.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 1.0ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.9ms\n",
      "Speed: 0.9ms preprocess, 2.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 1.3ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.9ms\n",
      "Speed: 0.9ms preprocess, 2.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 3.5ms\n",
      "Speed: 0.9ms preprocess, 3.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 1.3ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 3 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 3.5ms\n",
      "Speed: 0.8ms preprocess, 3.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 lizards, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.9ms\n",
      "Speed: 0.9ms preprocess, 2.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 1.4ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.4ms\n",
      "Speed: 0.7ms preprocess, 2.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.3ms\n",
      "Speed: 0.7ms preprocess, 2.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.8ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.9ms\n",
      "Speed: 0.8ms preprocess, 2.9ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 lizards, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.5ms\n",
      "Speed: 0.7ms preprocess, 2.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.8ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.7ms\n",
      "Speed: 0.9ms preprocess, 2.7ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 lizard, 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Classification Report (IoU ≥ 0.5 matched):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9540    0.9540    0.9540       174\n",
      "           1     0.9186    0.9133    0.9159       173\n",
      "           2     0.9135    0.9235    0.9185       183\n",
      "           3     0.9556    0.9663    0.9609       178\n",
      "           4     0.9818    0.9643    0.9730       168\n",
      "\n",
      "    accuracy                         0.9441       876\n",
      "   macro avg     0.9447    0.9443    0.9445       876\n",
      "weighted avg     0.9442    0.9441    0.9441       876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python pipeline_custom_eval.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lizard_class]",
   "language": "python",
   "name": "conda-env-lizard_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
